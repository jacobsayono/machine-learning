{"cells":[{"cell_type":"markdown","metadata":{},"source":["Jacob Sayono\n","\n","505368811\n","\n","CS M146 HW 3\n","\n","# Problem 1\n","\n","#### (a)\n","\n","TRUE\n","\n","In Support Vector Machines (SVM), only a subset of the training data affects the decision boundary. These data points are known as support vectors. Data points that are not support vectors do not influence the hyperplane parameters (i.e., they are not on the margin or do not violate the margin). Therefore, removing these non-support vector points from the training dataset would not change the solution of the SVM, implying that we can indeed eliminate some training points and still get the same solution.\n","\n","#### (b)\n","\n","TRUE\n","\n","In a soft-margin SVM formulation, the objective is to find the hyperplane parameters $(w, b)$ and the slack variables $\\xi_i$ for each data sample. Here, $w$ is a $d$-dimensional vector (one variable for each feature), $b$ is the bias term (one variable), and $\\xi_i$ are the slack variables (one for each of the $n$ samples). Thus, the total number of variables optimized is $d + 1 + n$.\n","\n","#### (c)\n","\n","TRUE\n","\n","AdaBoost adjusts the weights of training samples based on their classification in the previous rounds. Samples that are consistently correctly classified receive exponentially decreasing weights, as the algorithm focuses more on the harder-to-classify samples. Therefore, samples that have been correctly classified in all previous rounds end up with the lowest weights, as their correct classification suggests that they are 'easy' cases for the classifiers.\n","\n","#### (d)\n","\n","- **(i)** FALSE. Samples with $\\xi_i = 0$ are correctly classified and outside the margin. They do not affect the positioning of the hyperplane and thus are not support vectors.\n","- **(ii-iv)** TRUE. Samples with $0 < \\xi_i$ are on the wrong side of the margin but still contribute to the loss term in the SVM objective. These include samples within the margin ($0 < \\xi_i \\leq 1$) and those misclassified ($\\xi_i > 1$). Since they influence the loss term, they are support vectors.\n","\n","#### (e)\n","\n","- **(i)** FALSE. AdaBoost assigns a lower weight to samples that were correctly classified, as it focuses on correcting misclassifications.\n","- **(ii)** FALSE. AdaBoost assigns a higher weight to weak classifiers with higher error rates during classifier combination, not lower.\n","- **(iii)** FALSE. Samples that were incorrectly classified in the previous round are assigned higher weights, but not necessarily equal weights, as their weights depend on their individual errors and the performance of the classifier.\n","- **(iv)** TRUE. Since all the other options are false, this is the correct answer.\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Problem 2\n","\n","#### (a)\n","\n","YES\n","\n","The optimization problem P1, defined as $\\min_w \\left\\|w\\right\\|^2 + \\lambda \\sum_{i=1}^m \\xi_i^2$, subject to $y_i w^T x_i \\geq 1 - \\xi_i$ and $\\xi_i \\geq 0$ for all $i \\in [m]$, is convex. The objective function $\\left\\|w\\right\\|^2$ is convex as it is a quadratic form. The term $\\lambda \\sum_{i=1}^m \\xi_i^2$ is also convex since the sum of convex functions (squared terms) remains convex. The constraints are linear, thus preserving convexity. Therefore, both the objective and constraints are convex, making the entire optimization problem convex.\n","\n","#### (b)\n","\n","$$\n","L(w, \\xi, \\alpha, \\beta) = \\left\\|w\\right\\|^2 + \\lambda \\sum_{i=1}^m \\xi_i^2 + \\sum_{i=1}^m \\alpha_i (1 - \\xi_i - y_i w^T x_i) + \\sum_{i=1}^m \\beta_i (-\\xi_i)\n","$$\n","\n","Where $\\alpha$ and $\\beta$ are the vectors of Lagrange multipliers for the inequality constraints $y_i w^T x_i \\geq 1 - \\xi_i$ and $\\xi_i \\geq 0$, respectively.\n","\n","#### (c)\n","\n","$$\n","\\frac{\\partial L}{\\partial w} = 2w - \\sum_{i=1}^m \\alpha_i y_i x_i\n","$$\n","\n","Set this derivative to zero to solve for $w$ in terms of $\\alpha$:\n","\n","$$\n","2w = \\sum_{i=1}^m \\alpha_i y_i x_i \\implies w = \\frac{1}{2} \\sum_{i=1}^m \\alpha_i y_i x_i\n","$$\n","\n","#### (d)\n","\n","To find the derivative of the Lagrangian with respect to $\\xi_i$, compute:\n","\n","$$\n","\\frac{\\partial L}{\\partial \\xi_i} = 2 \\lambda \\xi_i - \\alpha_i - \\beta_i\n","$$\n","\n","Set this derivative to zero to solve for $\\xi_i$ in terms of $\\alpha_i$ and $\\beta_i$:\n","\n","$$\n","2 \\lambda \\xi_i = \\alpha_i + \\beta_i \\implies \\xi_i = \\frac{\\alpha_i + \\beta_i}{2 \\lambda}\n","$$\n","\n","#### (e)\n","Using the solutions for $w$ and $\\xi_i$ found in parts (c) and (d), substitute these back into the Lagrangian:\n","\n","$$\n","L\\left(\\frac{1}{2} \\sum_{i=1}^m \\alpha_i y_i x_i, \\frac{\\alpha_i + \\beta_i}{2 \\lambda}, \\alpha, \\beta\\right) = \\left\\|\\frac{1}{2} \\sum_{i=1}^m \\alpha_i y_i x_i\\right\\|^2 + \\lambda \\sum_{i=1}^m \\left(\\frac{\\alpha_i + \\beta_i}{2 \\lambda}\\right)^2 + \\text{substituting constraints}\n","$$\n","\n","Evaluating this function gives us the dual function $g(\\alpha, \\beta)$ which needs to be maximized with respect to $\\alpha$ and $\\beta$ under the conditions $\\alpha_i, \\beta_i \\geq 0$.\n","\n","#### (f)\n","\n","The primal variable $w$ can be expressed as a linear combination of the training examples $x_i$, weighted by the corresponding $\\alpha_i y_i$. The dual problem does not depend explicitly on the feature vectors $x_i$ but only through dot products between pairs of $x_i$. Thus, we can replace these dot products by a kernel function $K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)$, which computes dot products in a potentially higher-dimensional space without explicitly mapping the vectors to this space. This makes the solution kernelizable, allowing the use of the kernel trick for non-linear classification boundaries.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Problem 3\n","\n","#### Part (a) and (b)\n","\n","Table 1.\n","\n","| i | $x_1$ | $x_2$ | Label | $w_0$ | $h_1 \\equiv \\epsilon_1$ | $\\beta_1$ | $w_1$ | $h_2 \\equiv \\epsilon_2$ | $\\beta_2$ |\n","|---|-------|-------|-------|-------|-------------------------|-----------|-------|-------------------------|-----------|\n","| 1 | 2     | -1    | +     | 1/5   | 0.4                     | 0.203     | 0.25  | 0.5                     | 0.0       |\n","| 2 | 3     | 1     | +     | 1/5   | 0.4                     | 0.203     | 0.25  | 0.5                     | 0.0       |\n","| 3 | 2     | 4     | -     | 1/5   | 0.4                     | 0.203     | 0.167 | 0.5                     | 0.0       |\n","| 4 | 1     | 4     | -     | 1/5   | 0.4                     | 0.203     | 0.167 | 0.5                     | 0.0       |\n","| 5 | -1    | 3     | -     | 1/5   | 0.4                     | 0.203     | 0.167 | 0.5                     | 0.0       |"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Firt Iteration - Optimal Theta: 5.037593984962406\n","Firt Iteration - Weighted Error (epsilon1): 0.4\n","Firt Iteration - Coefficient (beta1): 0.2027325540540821\n","Firt Iteration - Updated Weights (w1): [0.25       0.25       0.16666667 0.16666667 0.16666667]\n","Second Iteration - Optimal Theta: -10.0\n","Second Iteration - Weighted Error (epsilon2): 0.5\n","Second Iteration - Coefficient (beta2): 0.0\n","Second Iteration - Updated Weights (w2): [0.25       0.25       0.16666667 0.16666667 0.16666667]\n"]}],"source":["# part (a) and (b)\n","\n","import numpy as np\n","\n","x1 = np.array([2, 3, 2, 1, -1])\n","x2 = np.array([-1, 1, 4, 4, 3])\n","labels = np.array([1, 1, -1, -1, -1])\n","w0 = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n","\n","def hypothesis(x1, x2, theta):\n","    return np.sign(-2 * x1 + x2 - theta)\n","\n","def weighted_error(theta, x1, x2, labels, weights):\n","    predictions = hypothesis(x1, x2, theta)\n","    errors = (predictions != labels).astype(float)\n","    weighted_errors = weights @ errors\n","    return weighted_errors\n","\n","# first iteration calculations\n","theta_values = np.linspace(-10, 10, 400)\n","errors = [weighted_error(theta, x1, x2, labels, w0) for theta in theta_values]\n","min_error_idx = np.argmin(errors)\n","theta_optimal1 = theta_values[min_error_idx]\n","epsilon1 = errors[min_error_idx]\n","beta1 = 0.5 * np.log((1 - epsilon1) / epsilon1)\n","w1 = w0 * np.exp(-beta1 * labels * hypothesis(x1, x2, theta_optimal1))\n","w1 /= np.sum(w1)\n","\n","print(f\"Firt Iteration - Optimal Theta: {theta_optimal1}\")\n","print(f\"Firt Iteration - Weighted Error (epsilon1): {epsilon1}\")\n","print(f\"Firt Iteration - Coefficient (beta1): {beta1}\")\n","print(f\"Firt Iteration - Updated Weights (w1): {w1}\")\n","\n","# second iteration calculations\n","errors = [weighted_error(theta, x1, x2, labels, w1) for theta in theta_values]\n","min_error_idx = np.argmin(errors)\n","theta_optimal2 = theta_values[min_error_idx]\n","epsilon2 = errors[min_error_idx]\n","beta2 = 0.5 * np.log((1 - epsilon2) / epsilon2)\n","w2 = w1 * np.exp(-beta2 * labels * hypothesis(x1, x2, theta_optimal2))\n","w2 /= np.sum(w2)\n","\n","print(f\"Second Iteration - Optimal Theta: {theta_optimal2}\")\n","print(f\"Second Iteration - Weighted Error (epsilon2): {epsilon2}\")\n","print(f\"Second Iteration - Coefficient (beta2): {beta2}\")\n","print(f\"Second Iteration - Updated Weights (w2): {w2}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Part (c) and (d)\n","\n","Table 2.\n","\n","Due to the lack of changes in the weight distribution from the first iteration and the practical limits of updating in case of a perfect classification, the second iteration could not effectively proceed.\n","\n","| i | $x_1$ | $x_2$ | Label | $w_0$ | $h_1 \\equiv \\epsilon_1$ | $\\beta_1$ | $w_1$ | $h_2 \\equiv \\epsilon_2$ | $\\beta_2$ |\n","|---|-------|-------|-------|-------|-------------------------|-----------|-------|-------------------------|-----------|\n","| 1 | 2     | -1    | +     | 1/5   | 0.0                     | $\\infty$  | 1/5   |                         |           |\n","| 2 | 3     | 1     | +     | 1/5   | 0.0                     | $\\infty$  | 1/5   |                         |           |\n","| 3 | 2     | 4     | -     | 1/5   | 0.0                     | $\\infty$  | 1/5   |                         |           |\n","| 4 | 1     | 4     | -     | 1/5   | 0.0                     | $\\infty$  | 1/5   |                         |           |\n","| 5 | -1    | 3     | -     | 1/5   | 0.0                     | $\\infty$  | 1/5   |                         |           |"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["First Iteration - Optimal Theta: 0.025062656641603454\n","First Iteration - Weighted Error (epsilon1): 0.0\n","First Iteration - Coefficient (beta1): inf\n","First Iteration - Updated Weights (w1): [nan nan nan nan nan]\n","Second Iteration - Optimal Theta: -10.0\n","Second Iteration - Weighted Error (epsilon2): nan\n","Second Iteration - Coefficient (beta2): nan\n","Second Iteration - Updated Weights (w2): [nan nan nan nan nan]\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_7536/1025930726.py:24: RuntimeWarning: divide by zero encountered in double_scalars\n","  beta = 0.5 * np.log((1 - epsilon) / epsilon)\n","/tmp/ipykernel_7536/1025930726.py:26: RuntimeWarning: invalid value encountered in divide\n","  w_new /= np.sum(w_new)\n"]}],"source":["# part (c) and (d)\n","\n","import numpy as np\n","\n","x1 = np.array([2, 3, 2, 1, -1])\n","x2 = np.array([-1, 1, 4, 4, 3])\n","labels = np.array([1, 1, -1, -1, -1])\n","w0 = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n","\n","def hypothesis_h2(x1, x2, theta):\n","    return np.sign(2 * x1 - x2 - theta)\n","\n","def weighted_error_h2(theta, x1, x2, labels, weights):\n","    predictions = hypothesis_h2(x1, x2, theta)\n","    errors = (predictions != labels).astype(float)\n","    weighted_errors = weights @ errors\n","    return weighted_errors\n","\n","def adaboost_iteration(x1, x2, labels, w, theta_values):\n","    errors = [weighted_error_h2(theta, x1, x2, labels, w) for theta in theta_values]\n","    min_error_idx = np.argmin(errors)\n","    theta_optimal = theta_values[min_error_idx]\n","    epsilon = errors[min_error_idx]\n","    beta = 0.5 * np.log((1 - epsilon) / epsilon)\n","    w_new = w * np.exp(-beta * labels * hypothesis_h2(x1, x2, theta_optimal))\n","    w_new /= np.sum(w_new)\n","    return theta_optimal, epsilon, beta, w_new\n","\n","# first iteration\n","theta_values = np.linspace(-10, 10, 400)\n","theta1, epsilon1, beta1, w1 = adaboost_iteration(x1, x2, labels, w0, theta_values)\n","\n","# second iteration\n","theta2, epsilon2, beta2, w2 = adaboost_iteration(x1, x2, labels, w1, theta_values)\n","\n","print(f\"First Iteration - Optimal Theta: {theta1}\")\n","print(f\"First Iteration - Weighted Error (epsilon1): {epsilon1}\")\n","print(f\"First Iteration - Coefficient (beta1): {beta1}\")\n","print(f\"First Iteration - Updated Weights (w1): {w1}\")\n","\n","print(f\"Second Iteration - Optimal Theta: {theta2}\")\n","print(f\"Second Iteration - Weighted Error (epsilon2): {epsilon2}\")\n","print(f\"Second Iteration - Coefficient (beta2): {beta2}\")\n","print(f\"Second Iteration - Updated Weights (w2): {w2}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7H420KxmCjKH"},"outputs":[],"source":["import os\n","import sys"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCDUFCh7Fd-n"},"outputs":[],"source":["# To add your own Drive Run this cell.\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQXiXrbaF3NK"},"outputs":[],"source":["# Please append your own directory after ‘/content/drive/My Drive/'\n","### ========== TODO : START ========== ###\n","sys.path += ['/content/drive/My Drive/cm146-spring23/hw3/HW3-code']\n","### ========== TODO : END ========== ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_OLupUPC2U3"},"outputs":[],"source":["\"\"\"\n","Author      : Yi-Chieh Wu, Sriram Sankararman\n","Description : Twitter\n","\"\"\"\n","\n","from string import punctuation\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# !!! MAKE SURE TO USE LinearSVC.decision_function(X), NOT LinearSVC.predict(X) !!!\n","# (this makes ''continuous-valued'' predictions)\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn import metrics"]},{"cell_type":"markdown","metadata":{"id":"47L2XVzBX6c5"},"source":["# Problem 4: Twitter Analysis Using SVM"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"9Z8E5YL0CzWe"},"outputs":[],"source":["######################################################################\n","# functions -- input/output\n","######################################################################\n","\n","def read_vector_file(fname):\n","    \"\"\"\n","    Reads and returns a vector from a file.\n","\n","    Parameters\n","    --------------------\n","        fname  -- string, filename\n","\n","    Returns\n","    --------------------\n","        labels -- numpy array of shape (n,)\n","                    n is the number of non-blank lines in the text file\n","    \"\"\"\n","    return np.genfromtxt(fname)\n","\n","\n","def write_label_answer(vec, outfile):\n","    \"\"\"\n","    Writes your label vector to the given file.\n","\n","    Parameters\n","    --------------------\n","        vec     -- numpy array of shape (n,) or (n,1), predicted scores\n","        outfile -- string, output filename\n","    \"\"\"\n","\n","    # for this project, you should predict 70 labels\n","    if(vec.shape[0] != 70):\n","        print(\"Error - output vector should have 70 rows.\")\n","        print(\"Aborting write.\")\n","        return\n","\n","    np.savetxt(outfile, vec)\n","    "]},{"cell_type":"code","execution_count":15,"metadata":{"id":"i67aTAmrGGHi"},"outputs":[],"source":["import string\n","######################################################################\n","# functions -- feature extraction\n","######################################################################\n","\n","def extract_words(input_string):\n","    \"\"\"\n","    Processes the input_string, separating it into \"words\" based on the presence\n","    of spaces, and separating punctuation marks into their own words.\n","\n","    Parameters\n","    --------------------\n","        input_string -- string of characters\n","\n","    Returns\n","    --------------------\n","        words        -- list of lowercase \"words\"\n","    \"\"\"\n","\n","    for c in string.punctuation :\n","        input_string = input_string.replace(c, ' ' + c + ' ')\n","    return input_string.lower().split()\n","\n","\n","def extract_dictionary(infile):\n","    \"\"\"\n","    Given a filename, reads the text file and builds a dictionary of unique\n","    words/punctuations.\n","\n","    Parameters\n","    --------------------\n","        infile    -- string, filename\n","\n","    Returns\n","    --------------------\n","        word_list -- dictionary, (key, value) pairs are (word, index)\n","    \"\"\"\n","\n","    word_list = {}\n","    idx = 0\n","    with open(infile, 'r') as fid :\n","        # process each line to populate word_list\n","        for input_string in fid:\n","            words = extract_words(input_string)\n","            for word in words:\n","                if word not in word_list:\n","                    word_list[word] = idx\n","                    idx += 1\n","    return word_list\n","\n","\n","def extract_feature_vectors(infile, word_list):\n","    \"\"\"\n","    Produces a bag-of-words representation of a text file specified by the\n","    filename infile based on the dictionary word_list.\n","\n","    Parameters\n","    --------------------\n","        infile         -- string, filename\n","        word_list      -- dictionary, (key, value) pairs are (word, index)\n","\n","    Returns\n","    --------------------\n","        feature_matrix -- numpy array of shape (n,d)\n","                          boolean (0,1) array indicating word presence in a string\n","                            n is the number of non-blank lines in the text file\n","                            d is the number of unique words in the text file\n","    \"\"\"\n","\n","    num_lines = sum(1 for line in open(infile,'r'))\n","    num_words = len(word_list)\n","    feature_matrix = np.zeros((num_lines, num_words))\n","\n","    with open(infile, 'r') as fid :\n","        # process each line to populate feature_matrix\n","        for i, input_string in enumerate(fid):\n","            words = extract_words(input_string)\n","            for word in words:\n","                feature_matrix[i, word_list[word]] = 1.0\n","\n","    return feature_matrix"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"-MvTxQPRGOOf"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n","from sklearn.model_selection import cross_val_score\n","from sklearn.svm import LinearSVC\n","import numpy as np\n","\n","######################################################################\n","# functions -- evaluation\n","######################################################################\n","\n","def performance(y_true, y_pred, metric=\"accuracy\"):\n","    \"\"\"\n","    Calculates the performance metric based on the agreement between the\n","    true labels and the predicted labels.\n","\n","    Parameters\n","    --------------------\n","        y_true -- numpy array of shape (n,), known labels\n","        y_pred -- numpy array of shape (n,), (continuous-valued) predictions\n","        metric -- string, option used to select the performance measure\n","                  options: 'accuracy', 'f1-score', 'auroc', 'precision',\n","                           'sensitivity', 'specificity'\n","\n","    Returns\n","    --------------------\n","        score  -- float, performance score\n","    \"\"\"\n","    # map continuous-valued predictions to binary labels\n","    y_label = np.sign(y_pred)\n","    y_label[y_label==0] = 1\n","\n","    ### ========== TODO : START ========== ###\n","    # part 1a: compute classifier performance\n","    if metric == \"accuracy\":\n","        return accuracy_score(y_true, y_label)\n","    elif metric == \"f1-score\":\n","        return f1_score(y_true, y_label)\n","    elif metric == \"auroc\":\n","        return roc_auc_score(y_true, y_pred)  # AUROC expects score, not labels\n","    elif metric == \"precision\":\n","        return precision_score(y_true, y_label)\n","    elif metric == \"sensitivity\":\n","        return recall_score(y_true, y_label)\n","    elif metric == \"specificity\":\n","        tn, fp, _, _ = confusion_matrix(y_true, y_label).ravel()\n","        return tn / (tn + fp)\n","    else:\n","        raise ValueError(\"Unknown metric.\")\n","    ### ========== TODO : END ========== ###\n","\n","from sklearn.metrics import make_scorer\n","def specificity_score(y_true, y_pred):\n","    tn, fp, _, _ = confusion_matrix(y_true, y_pred).ravel()\n","    return tn / (tn + fp)\n","\n","specificity_scorer = make_scorer(specificity_score, greater_is_better=True)\n","\n","\n","def cv_performance(clf, X, y, kf, metric=\"accuracy\"):\n","    \"\"\"\n","    Splits the data, X and y, into k-folds and runs k-fold cross-validation.\n","    Trains classifier on k-1 folds and tests on the remaining fold.\n","    Calculates the k-fold cross-validation performance metric for classifier\n","    by averaging the performance across folds.\n","\n","    Parameters\n","    --------------------\n","        clf    -- classifier (instance of LinearSVC)\n","        X      -- numpy array of shape (n,d), feature vectors\n","                    n = number of examples\n","                    d = number of features\n","        y      -- numpy array of shape (n,), binary labels {1,-1}\n","        kf     -- model_selection.StratifiedKFold\n","        metric -- string, option used to select performance measure\n","\n","    Returns\n","    --------------------\n","        score   -- float, average cross-validation performance across k folds\n","    \"\"\"\n","\n","    ### ========== TODO : START ========== ###\n","    # part 1b: compute average cross-validation performance\n","    if metric == \"specificity\":\n","        score = cross_val_score(clf, X, y, scoring=specificity_scorer, cv=kf)\n","    else:\n","        score = cross_val_score(clf, X, y, scoring=metric, cv=kf)\n","    return score.mean()\n","    ### ========== TODO : END ========== ###\n","\n","\n","def select_param_linear(X, y, kf, metric=\"accuracy\"):\n","    \"\"\"\n","    Sweeps different settings for the hyperparameter of a linear SVM,\n","    calculating the k-fold CV performance for each setting, then selecting the\n","    hyperparameter that 'maximize' the average k-fold CV performance.\n","\n","    Parameters\n","    --------------------\n","        X      -- numpy array of shape (n,d), feature vectors\n","                    n = number of examples\n","                    d = number of features\n","        y      -- numpy array of shape (n,), binary labels {1,-1}\n","        kf     -- model_selection.StratifiedKFold\n","        metric -- string, option used to select performance measure\n","\n","    Returns\n","    --------------------\n","        C -- float, optimal parameter value for linear SVM\n","    \"\"\"\n","\n","    print('Linear SVM Hyperparameter Selection based on ' + str(metric) + ':')\n","    C_range = 10.0 ** np.arange(-3, 3)\n","\n","    ### ========== TODO : START ========== ###\n","    # part 1c: select optimal hyperparameter using cross-validation\n","    best_score = -1\n","    best_C = None\n","\n","    for C in C_range:\n","        clf = LinearSVC(loss='hinge', random_state=0, C=C)\n","        score = cv_performance(clf, X, y, kf, metric=metric)\n","        if score > best_score:\n","            best_score = score\n","            best_C = C\n","\n","    return best_C\n","    ### ========== TODO : END ========== ###\n","\n","\n","def performance_test(clf, X, y, metric=\"accuracy\"):\n","    \"\"\"\n","    Estimates the performance of the classifier.\n","\n","    Parameters\n","    --------------------\n","        clf          -- classifier (instance of LinearSVC)\n","                          [already fit to data]\n","        X            -- numpy array of shape (n,d), feature vectors of test set\n","                          n = number of examples\n","                          d = number of features\n","        y            -- numpy array of shape (n,), binary labels {1,-1} of test set\n","        metric       -- string, option used to select performance measure\n","\n","    Returns\n","    --------------------\n","        score        -- float, classifier performance\n","    \"\"\"\n","\n","\n","    ### ========== TODO : START ========== ###\n","    # part 2b: return performance on test data under a metric.\n","    y_scores = clf.decision_function(X)\n","\n","    if metric in [\"accuracy\", \"f1\", \"precision\", \"recall\", \"specificity\"]:\n","        y_pred = np.sign(y_scores)\n","        y_pred[y_pred == 0] = 1  # Treat zero as positive\n","\n","        if metric == \"accuracy\":\n","            return accuracy_score(y, y_pred)\n","        elif metric == \"f1\":\n","            return f1_score(y, y_pred)\n","        elif metric == \"precision\":\n","            return precision_score(y, y_pred)\n","        elif metric == \"recall\":\n","            return recall_score(y, y_pred)\n","        elif metric == \"specificity\":\n","            tn, fp, _, _ = confusion_matrix(y, y_pred).ravel()\n","            return tn / (tn + fp)\n","    elif metric == \"roc_auc\":\n","        return roc_auc_score(y, y_scores)  # AUROC expects scores, not binary labels\n","    else:\n","        raise ValueError(\"Unknown metric: {}\".format(metric))\n","    ### ========== TODO : END ========== ###"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"zMIQRGpYErVF"},"outputs":[{"name":"stdout","output_type":"stream","text":["1811\n","Linear SVM Hyperparameter Selection based on accuracy:\n","Best C for accuracy: 10.0\n","Linear SVM Hyperparameter Selection based on f1:\n","Best C for f1: 10.0\n","Linear SVM Hyperparameter Selection based on roc_auc:\n","Best C for roc_auc: 10.0\n","Linear SVM Hyperparameter Selection based on precision:\n","Best C for precision: 10.0\n","Linear SVM Hyperparameter Selection based on recall:\n","Best C for recall: 0.001\n","Linear SVM Hyperparameter Selection based on specificity:\n","Best C for specificity: 10.0\n","Performance on test data for accuracy: 0.7428571428571429\n","Performance on test data for f1: 0.43749999999999994\n","Performance on test data for roc_auc: 0.7453838678328474\n","Performance on test data for precision: 0.6363636363636364\n","Performance on test data for recall: 1.0\n","Performance on test data for specificity: 0.9183673469387755\n"]}],"source":["from sklearn.model_selection import StratifiedKFold\n","######################################################################\n","# main\n","######################################################################\n","\n","def main() :\n","    np.random.seed(1234)\n","\n","    # read the tweets and its labels, change the following two lines to your own path.\n","    ### ========== TODO : START ========== ###\n","    file_path = '../data/tweets.txt'\n","    label_path = '../data/labels.txt'\n","    ### ========== TODO : END ========== ###\n","    dictionary = extract_dictionary(file_path)\n","    print(len(dictionary))\n","    X = extract_feature_vectors(file_path, dictionary)\n","    y = read_vector_file(label_path)\n","    # split data into training (training + cross-validation) and testing set\n","    X_train, X_test = X[:560], X[560:]\n","    y_train, y_test = y[:560], y[560:]\n","\n","    metric_list = [\"accuracy\", \"f1\", \"roc_auc\", \"precision\", \"recall\", \"specificity\"]\n","\n","    ### ========== TODO : START ========== ###\n","    # part 1b: create stratified folds (5-fold CV)\n","    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n","    best_C = {}\n","\n","    # part 1c: for each metric, select optimal hyperparameter for linear SVM using CV\n","    for metric in metric_list:\n","        optimal_C = select_param_linear(X_train, y_train, kf, metric=metric)\n","        best_C[metric] = optimal_C\n","        print(f\"Best C for {metric}: {optimal_C}\")\n","\n","    # part 2a: train linear SVMs with selected hyperparameters\n","    classifiers = {}\n","    for metric, C in best_C.items():\n","        clf = LinearSVC(loss='hinge', random_state=0, C=C)\n","        clf.fit(X_train, y_train)\n","        classifiers[metric] = clf\n","\n","    # part 2b: test the performance of your classifiers.\n","    performances = {}\n","    for metric, clf in classifiers.items():\n","        score = performance_test(clf, X_test, y_test, metric=metric)\n","        performances[metric] = score\n","        print(f\"Performance on test data for {metric}: {score}\")\n","    \n","    ### ========== TODO : END ========== ###\n","\n","\n","if __name__ == \"__main__\" :\n","    main()"]},{"cell_type":"markdown","metadata":{"id":"_W-_mjX0JMes"},"source":["# Problem 5: Boosting vs. Decision Tree"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"0uzCdPTkOQSY"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics\n","from sklearn.model_selection import cross_val_score, train_test_split"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"DVxef2sxOmVI"},"outputs":[],"source":["class Data :\n","    \n","    def __init__(self) :\n","        \"\"\"\n","        Data class.\n","        \n","        Attributes\n","        --------------------\n","            X -- numpy array of shape (n,d), features\n","            y -- numpy array of shape (n,), targets\n","        \"\"\"\n","                \n","        # n = number of examples, d = dimensionality\n","        self.X = None\n","        self.y = None\n","        \n","        self.Xnames = None\n","        self.yname = None\n","    \n","    def load(self, filename, header=0, predict_col=-1) :\n","        \"\"\"Load csv file into X array of features and y array of labels.\"\"\"\n","        \n","        # determine filename\n","        f = filename\n","        \n","        # load data\n","        with open(f, 'r') as fid :\n","            data = np.loadtxt(fid, delimiter=\",\", skiprows=header)\n","        \n","        # separate features and labels\n","        if predict_col is None :\n","            self.X = data[:,:]\n","            self.y = None\n","        else :\n","            if data.ndim > 1 :\n","                self.X = np.delete(data, predict_col, axis=1)\n","                self.y = data[:,predict_col]\n","            else :\n","                self.X = None\n","                self.y = data[:]\n","        \n","        # load feature and label names\n","        if header != 0:\n","            with open(f, 'r') as fid :\n","                header = fid.readline().rstrip().split(\",\")\n","                \n","            if predict_col is None :\n","                self.Xnames = header[:]\n","                self.yname = None\n","            else :\n","                if len(header) > 1 :\n","                    self.Xnames = np.delete(header, predict_col)\n","                    self.yname = header[predict_col]\n","                else :\n","                    self.Xnames = None\n","                    self.yname = header[0]\n","        else:\n","            self.Xnames = None\n","            self.yname = None\n","\n","\n","# helper functions\n","def load_data(filename, header=0, predict_col=-1) :\n","    \"\"\"Load csv file into Data class.\"\"\"\n","    data = Data()\n","    data.load(filename, header=header, predict_col=predict_col)\n","    return data"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"_Zcf4WVqJSpe"},"outputs":[],"source":["# Change the path to your own data directory\n","### ========== TODO : START ========== ###\n","titanic = load_data(\"../data/titanic_train.csv\", header=1, predict_col=0)\n","### ========== TODO : END ========== ###\n","X = titanic.X; Xnames = titanic.Xnames\n","y = titanic.y; yname = titanic.yname\n","n,d = X.shape  # n = number of examples, d =  number of features"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"3Ta7XHRWQGNo"},"outputs":[],"source":["def error(clf, X, y, ntrials=100, test_size=0.2) :\n","    \"\"\"\n","    Computes the classifier error over a random split of the data,\n","    averaged over ntrials runs.\n","\n","    Parameters\n","    --------------------\n","        clf         -- classifier\n","        X           -- numpy array of shape (n,d), features values\n","        y           -- numpy array of shape (n,), target classes\n","        ntrials     -- integer, number of trials\n","        test_size   -- proportion of data used for evaluation\n","\n","    Returns\n","    --------------------\n","        train_error -- float, training error\n","        test_error  -- float, test error\n","    \"\"\"\n","\n","    train_error = 0\n","    test_error = 0\n","\n","    train_scores = []; test_scores = [];\n","    for i in range(ntrials):\n","        xtrain, xtest, ytrain, ytest = train_test_split (X,y, test_size = test_size, random_state = i)\n","        clf.fit (xtrain, ytrain)\n","\n","        ypred = clf.predict (xtrain)\n","        err = 1 - metrics.accuracy_score (ytrain, ypred, normalize = True)\n","        train_scores.append (err)\n","\n","        ypred = clf.predict (xtest)\n","        err = 1 - metrics.accuracy_score (ytest, ypred, normalize = True)\n","        test_scores.append (err)\n","\n","    train_error =  np.mean (train_scores)\n","    test_error = np.mean (test_scores)\n","    return train_error, test_error\n"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"W8-U3un5PjGq"},"outputs":[{"name":"stdout","output_type":"stream","text":["Classifying using Decision Tree...\n","Training Error (DT): 0.014044943820224698\n"]}],"source":["### ========== TODO : START ========== ###\n","# Part 4(a): Implement the decision tree classifier and report the training error.\n","print('Classifying using Decision Tree...')\n","dt_clf = DecisionTreeClassifier(criterion='entropy', random_state=0)\n","dt_clf.fit(X, y)\n","\n","y_pred_train = dt_clf.predict(X)\n","train_error = 1 - metrics.accuracy_score(y, y_pred_train)\n","print(f\"Training Error (DT): {train_error}\")\n","### ========== TODO : END ========== ###"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"_x_PevK8Q4dx"},"outputs":[{"name":"stdout","output_type":"stream","text":["Classifying using Random Forest...\n","Max Samples 10%: Train Error = 0.1357293497363796, Test Error = 0.19587412587412587\n","Max Samples 20%: Train Error = 0.10314586994727591, Test Error = 0.18797202797202794\n","Max Samples 30%: Train Error = 0.0818629173989455, Test Error = 0.18888111888111891\n","Max Samples 40%: Train Error = 0.05869947275922671, Test Error = 0.19216783216783218\n","Max Samples 50%: Train Error = 0.03388400702987697, Test Error = 0.19888111888111892\n","Max Samples 60%: Train Error = 0.017785588752196824, Test Error = 0.20111888111888113\n","Max Samples 70%: Train Error = 0.012390158172232001, Test Error = 0.20475524475524473\n","Max Samples 80%: Train Error = 0.011528998242530775, Test Error = 0.20671328671328676\n","Best Test Error: 0.18797202797202794 at 20% max_samples\n"]}],"source":["### ========== TODO : START ========== ###\n","# Part 4(b): Implement the random forest classifier and adjust the number of samples used in bootstrap sampling.\n","print('Classifying using Random Forest...')\n","best_test_error = float('inf')\n","best_max_samples = None\n","\n","for max_samples_percent in range(10, 90, 10):  # From 10% to 80%\n","    max_samples = int(n * (max_samples_percent / 100))\n","    rf_clf = RandomForestClassifier(criterion='entropy', random_state=0, max_samples=max_samples)\n","    train_error, test_error = error(rf_clf, X, y)\n","    print(f\"Max Samples {max_samples_percent}%: Train Error = {train_error}, Test Error = {test_error}\")\n","    \n","    if test_error < best_test_error:\n","        best_test_error = test_error\n","        best_max_samples = max_samples_percent\n","\n","print(f\"Best Test Error: {best_test_error} at {best_max_samples}% max_samples\")\n","### ========== TODO : END ========== ###"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"ZFUyPTPwT53v"},"outputs":[{"name":"stdout","output_type":"stream","text":["Classifying using Random Forest...\n","Max Features 1: Train Error = 0.10121265377855888, Test Error = 0.18776223776223777\n","Max Features 2: Train Error = 0.10314586994727591, Test Error = 0.18797202797202794\n","Max Features 3: Train Error = 0.10244288224956065, Test Error = 0.1872727272727273\n","Max Features 4: Train Error = 0.10430579964850617, Test Error = 0.1874125874125874\n","Max Features 5: Train Error = 0.10544815465729351, Test Error = 0.1886013986013986\n","Max Features 6: Train Error = 0.10581722319859402, Test Error = 0.189020979020979\n","Max Features 7: Train Error = 0.10776801405975397, Test Error = 0.18895104895104897\n","Best Test Error: 0.1872727272727273 at 3 max_features\n"]}],"source":["### ========== TODO : START ========== ###\n","# Part 4(c): Implement the random forest classifier and adjust the number of features for each decision tree.\n","print('Classifying using Random Forest...')\n","best_test_error = float('inf')\n","best_max_features = None\n","\n","best_max_samples = int(n * (best_max_samples / 100))\n","for max_features in range(1, d + 1):  # d is the number of features\n","    rf_clf = RandomForestClassifier(criterion='entropy', random_state=0, max_samples=best_max_samples, max_features=max_features)\n","    train_error, test_error = error(rf_clf, X, y)\n","    print(f\"Max Features {max_features}: Train Error = {train_error}, Test Error = {test_error}\")\n","    \n","    if test_error < best_test_error:\n","        best_test_error = test_error\n","        best_max_features = max_features\n","\n","print(f\"Best Test Error: {best_test_error} at {best_max_features} max_features\")\n","### ========== TODO : END ========== ###"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
