{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "from utils import mnist_reader\n",
    "from utils.data_load import load\n",
    "import codes\n",
    "# Load matplotlib images inline\n",
    "%matplotlib inline\n",
    "# These are important for reloading any code you write in external .py files.\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f78664",
   "metadata": {},
   "source": [
    "# Problem 4: Binary Classification\n",
    "\n",
    "Please follow our instructions in the same order to solve the binary classification problem.\n",
    "Please print out the entire results and codes when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train = mnist_reader.load_mnist('fashion-mnist/data/fashion', kind='train')\n",
    "#X_test, y_test = mnist_reader.load_mnist('fashion-mnist/data/fashion', kind='t10k')\n",
    "\n",
    "X_train = np.load('./data/binary_classification/X_train.npy')\n",
    "y_train = np.load('./data/binary_classification/y_train.npy')\n",
    "X_test = np.load('./data/binary_classification/X_test.npy')\n",
    "y_test = np.load('./data/binary_classification/y_test.npy')\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd01aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PART (a): \n",
    "# To Visualize a point in the dataset\n",
    "index = 11\n",
    "X = np.array(X_train[index], dtype='uint8')\n",
    "X = X.reshape((28, 28))\n",
    "fig = plt.figure()\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.show()\n",
    "if y_train[index] == 1:\n",
    "    label = 'Dress'\n",
    "else:\n",
    "    label = 'Shirt'\n",
    "print('label is', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9564321",
   "metadata": {},
   "source": [
    "## Train Perceptron\n",
    "In the following cells, you will build Perceptron Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a619575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART (b),(c): \n",
    "# Implement the perceptron Algorithm and compute the number of mis-classified point\n",
    "N = X_train.shape[0] # Number of data point train\n",
    "N_test = X_test.shape[0] # Number of data point test\n",
    "d = X_train.shape[1] # Number of features\n",
    "loss_hist = []\n",
    "W = np.zeros((d+1,1))\n",
    "X_train_h = np.hstack((np.ones((N,1)), X_train))\n",
    "X_test_h = np.hstack((np.ones((N_test,1)), X_test))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test accuracy in the same plot\n",
    "# for m range from 1 to N\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3f1e2",
   "metadata": {},
   "source": [
    "## Train Logistic Regression\n",
    "In the following cells, you will build a logistic regression. You will implement its loss function, then subsequently train it with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.Logistic import Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (f): \n",
    "X_train = np.load('./data/binary_classification/X_train.npy')\n",
    "y_train = np.load('./data/binary_classification/y_train.npy')\n",
    "X_test = np.load('./data/binary_classification/X_test.npy')\n",
    "y_test = np.load('./data/binary_classification/y_test.npy')\n",
    "## Complete loss_and_grad function in Logistic.py file and test your results.\n",
    "N,d = X_train.shape\n",
    "logistic = Logistic(d=d, reg_param=0)\n",
    "loss, grad = logistic.loss_and_grad(X_train,y_train)\n",
    "print('Loss function=',loss)\n",
    "print(np.linalg.norm(grad,ord=2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d983f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (g):\n",
    "## Complete train_LR function in Logisitc.py file\n",
    "loss_history, w = logistic.train_LR(X_train,y_train, eta=1e-6,batch_size=100, num_iters=5000)\n",
    "fig = plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()\n",
    "fig.savefig('./plots/LR_loss_hist.pdf')\n",
    "print('Weight squared norm',np.linalg.norm(w,ord=2)**2)\n",
    "print('Final loss',loss_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0270847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART (h)\n",
    "# Complete predict function in Logisitc.py file and compute the percentage of mis-classified points\n",
    "y_pred = logistic.predict(X_test)\n",
    "test_err = np.sum((y_test!=y_pred))*100/X_test.shape[0]\n",
    "print(test_err,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (i): \n",
    "Batch = [1, 50 , 100, 200, 300]\n",
    "test_err = np.zeros((len(Batch),1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# Train the Logistic regression for different batch size. Avergae the test error over 10 times\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "fig = plt.figure()\n",
    "plt.plot(Batch,test_err)\n",
    "plt.xlabel('Batch_size')\n",
    "plt.ylabel('Test_error')\n",
    "plt.show()\n",
    "fig.savefig('./plots/LR_Batch_test.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b02faa",
   "metadata": {},
   "source": [
    "# Problem 5: Linear Regression\n",
    "Please follow our instructions in the same order to solve the linear regresssion problem.\n",
    "\n",
    "Please print out the entire results and codes when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58463ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    Load the dataset from disk and perform preprocessing to prepare it for the linear regression problem.   \n",
    "    \"\"\"\n",
    "    X_train, y_train = load('./data/regression/regression_train.csv')\n",
    "    X_test, y_test = load('./data/regression/regression_test.csv')\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test= get_data()  \n",
    "\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (a): \n",
    "## Plot the training and test data ##\n",
    "\n",
    "plt.plot(X_train, y_train,'o', color='black')\n",
    "plt.plot(X_test, y_test,'o', color='blue')\n",
    "plt.xlabel('Train data')\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f811ca3b",
   "metadata": {},
   "source": [
    "## Training Linear Regression\n",
    "In the following cells, you will build a linear regression. You will implement its loss function, then subsequently train it with gradient descent. You will choose the learning rate of gradient descent to optimize its classification performance. Finally, you will get the opimal solution using closed form expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.Regression import Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1342f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (d): \n",
    "## Complete loss_and_grad function in Regression.py file and test your results.\n",
    "regression = Regression(m=1, reg_param=0)\n",
    "loss, grad = regression.loss_and_grad(X_train,y_train)\n",
    "print('Loss value',loss)\n",
    "print('Gradient value',grad)\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (e): \n",
    "## Complete train_LR function in Regression.py file \n",
    "loss_history, w = regression.train_LR(X_train,y_train, eta=1e-3,batch_size=30, num_iters=10000)\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss function')\n",
    "plt.show()\n",
    "print(w)\n",
    "print('Final loss:',loss_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c37de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (e) (Different Batch Sizes):\n",
    "from numpy.linalg import norm \n",
    "Batch = [1, 5, 15, 20, 25, 30]\n",
    "test_err = np.zeros((len(Batch),1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# Train the Logistic regression for different batch size Avergae the test error over 10 times\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "fig = plt.figure()\n",
    "plt.plot(Batch,test_err)\n",
    "plt.xlabel('Batch_size')\n",
    "plt.ylabel('Test_error')\n",
    "plt.show()\n",
    "fig.savefig('./plots/LR_Batch_test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (f): \n",
    "## Complete closed_form function in Regression.py file\n",
    "loss_2, w_2 = regression.closed_form(X_train, y_train)\n",
    "print('Optimal solution loss',loss_2)\n",
    "print('Optimal solution gradient',w_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PART (g): \n",
    "train_loss=np.zeros((10,1))\n",
    "test_loss=np.zeros((10,1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test loss in the same plot\n",
    "# for m range from 1 to 10\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART (j): \n",
    "train_loss=np.zeros((10,1))\n",
    "test_loss=np.zeros((10,1))\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "# complete the following code to plot both the training and test loss in the same plot\n",
    "# for m range from 1 to 10\n",
    "# ================================================================ #\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss, color='black')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
